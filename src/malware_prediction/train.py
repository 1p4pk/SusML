import logging

import torch


class ModelTrainer:

    def __init__(self, data_loader, model, loss, optimizer, model_path):
        self.data_loader = data_loader
        self.model = model
        self.loss_fn = loss
        self.optimizer = optimizer
        self.model_path = model_path

    def train(self, epochs, verbose=False):
        for epoch in range(epochs):
            avg_loss = self._train_step()

            if epoch != 0 and epoch % 10 == 0:
                # saving model: https://stackoverflow.com/a/49078976
                torch.save(self.model, self.model)
                print(f"Model saved on epoch {epoch}")

            if verbose and epoch % 2 == 1:
                print(f'Epoch: {epoch} Loss: {avg_loss}')

        torch.save(self.model, self.model_path)

    def _train_step(self):
        self.model.train()
        loss_sum = 0

        for idx, (train_data, train_labels) in enumerate(self.data_loader):
            logging.debug(f"Batch Nr: {idx}")
            train_data = train_data.requires_grad_()
            self.optimizer.zero_grad()
            y_pred = self.model(train_data)
            loss = self.loss_fn(y_pred.float().view(-1), train_labels)
            loss_sum += loss.item()
            loss.backward()
            self.optimizer.step()
        return loss_sum / (len(self.data_loader) * self.data_loader.batch_size)
