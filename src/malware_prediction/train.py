from torch import distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


class ModelTrainer:

    def __init__(self, model, loss, optimizer):
        self.model = model
        self.loss = loss
        self.optimizer = optimizer

    def train(self, epochs, x_train, y_train, verbose=False):
        for epoch in range(epochs):
            y_hat = self.model(x_train)
            output = self.loss(y_hat, y_train)

            if verbose and epoch % 2 == 1:
                print(f'Epoch: {epoch} Loss: {output}')

            self.optimizer.zero_grad()
            output.backward()
            self.optimizer.step()


class DPPModelTrainer(ModelTrainer):

    def __init__(self, model, loss, optimizer, rank, size):
        dist.init_process_group('mpi', rank=rank, world_size=size)
        super().__init__(DDP(model), loss, optimizer)

