import logging


class ModelTrainer:

    def __init__(self, data_loader, model, loss, optimizer):
        self.data_loader = data_loader
        self.model = model
        self.loss_fn = loss
        self.optimizer = optimizer

    def train(self, epochs, verbose=False):
        for epoch in range(epochs):
            avg_loss = self._train_step()

            if verbose and epoch % 2 == 1:
                print(f'Epoch: {epoch} Loss: {avg_loss}')

    def _train_step(self):
        self.model.train()
        loss_sum = 0

        for idx, (train_data, train_labels) in enumerate(self.data_loader):
            logging.debug(f"Batch Nr: {idx}")
            train_data = train_data.requires_grad_()
            self.optimizer.zero_grad()
            y_pred = self.model(train_data)
            loss = self.loss_fn(y_pred.float(), train_labels)
            loss_sum += loss.item()
            loss.backward()
            self.optimizer.step()
        loss_avg = loss_sum / (len(self.data_loader) * self.data_loader.batch_size)
        return loss_avg
