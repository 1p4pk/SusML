import os
import socket
import sys

from torch import distributed as dist
from torch import nn
from torch import optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.dataloader import DataLoader

from .dataset_train import MalwareTrainDataset
from .model import MalwarePredictor
from .train import ModelTrainer
from ..benchmark import timeit, memory_monitor, cpu_monitor, disk_monitor

SIZE = int(os.environ['OMPI_COMM_WORLD_SIZE'])
RANK = int(os.environ['OMPI_COMM_WORLD_RANK'])
HOSTNAME = socket.gethostname()


# LABEL_DATA_PATH = f'./data/tensor/label_data_{RANK}.pt'
# TRAIN_DATA_PATH = f'./data/tensor/train_data_{RANK}.pt'

LABEL_DATA_PATH = f'./data/tensor/label_data.pt'
TRAIN_DATA_PATH = f'./data/tensor/train_data.pt'

OPTIMIZER = sys.argv[2]
LOSS_FUNCTION = sys.argv[3]
MODEL_PATH = f"./models/model_{OPTIMIZER}_{LOSS_FUNCTION}.pb"


@cpu_monitor(file_name="model")
@memory_monitor(file_name="model")
@disk_monitor(file_name="model")
@timeit(file_name="model")
def run(backend='mpi'):
    """ Initialize the distributed environment. """
    print(f"Running rank {RANK} of {SIZE} on {HOSTNAME}")
    print("Load Data")
    dist.init_process_group(backend)
    dataset = MalwareTrainDataset(TRAIN_DATA_PATH, LABEL_DATA_PATH)
    loader = DataLoader(dataset, batch_size=100)
    model = DDP(MalwarePredictor(loader.dataset.cols, 100, 100, 1))
    loss = get_loss(LOSS_FUNCTION)
    optimizer = get_optimizer(OPTIMIZER, model.parameters())
    print("Model prepared.")
    print("Process group initialized.")
    trainer = ModelTrainer(loader, model, loss, optimizer, MODEL_PATH)
    print("Trainer initialized.")
    trainer.train(20, verbose=True)
    print("Done training.")
    dist.destroy_process_group()


def get_optimizer(optimizer: str, parameters):
    optimizer = optimizer.lower()
    if optimizer == "sgd":
        return optim.SGD(parameters, lr=0.01)
    elif optimizer == "adagrad":
        return optim.Adagrad(parameters, lr=0.01)
    elif optimizer == "adam":
        return optim.Adam(parameters, lr=0.001)
    elif optimizer == "adamw":
        return optim.AdamW(parameters, lr=0.001)
    elif optimizer == "asgd":
        return optim.ASGD(parameters, lr=0.01)


def get_loss(loss: str):
    loss = loss.lower()
    if loss == "mseloss":
        return nn.MSELoss()
    elif loss == "crossentropyloss":
        return nn.CrossEntropyLoss()
    elif loss == "bceloss":
        return nn.BCELoss()


if __name__ == '__main__':
    run()
